{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Deep Learning with TensorFlow: openSAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAP Innovation Center Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_POS_PATH = \"data/sentiment/rt-polarity-pos.txt\"\n",
    "TRAINING_NEG_PATH = \"data/sentiment/rt-polarity-neg.txt\"\n",
    "NUM_WORDS = 200\n",
    "TRAIN_RATIO = .8\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = .001\n",
    "NUM_LAYERS = 1\n",
    "NUM_UNITS = 10\n",
    "DROPOUT_PROB = .8\n",
    "EMBEDDING_SIZE = 10\n",
    "MODEL_PATH = \"dump/\"\n",
    "EVERY_N_ITER = 500\n",
    "CLEAN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean\n",
    "if CLEAN:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        shutil.rmtree(MODEL_PATH)\n",
    "        os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-05 17:43:43--  https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/polarityData/rt-polaritydata/rt-polarity-pos.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 626168 (611K) [text/plain]\n",
      "Saving to: './data/sentiment/rt-polarity-pos.txt'\n",
      "\n",
      "rt-polarity-pos.txt 100%[===================>] 611.49K  2.44MB/s    in 0.2s    \n",
      "\n",
      "2017-11-05 17:43:43 (2.44 MB/s) - './data/sentiment/rt-polarity-pos.txt' saved [626168/626168]\n",
      "\n",
      "--2017-11-05 17:43:43--  https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/polarityData/rt-polaritydata/rt-polarity-neg.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 612290 (598K) [text/plain]\n",
      "Saving to: './data/sentiment/rt-polarity-neg.txt'\n",
      "\n",
      "rt-polarity-neg.txt 100%[===================>] 597.94K  2.18MB/s    in 0.3s    \n",
      "\n",
      "2017-11-05 17:43:44 (2.18 MB/s) - './data/sentiment/rt-polarity-neg.txt' saved [612290/612290]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create data/sentiment folder, if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"data/sentiment\"):\n",
    "    os.makedirs(\"data/sentiment\")\n",
    "    \n",
    "#Download sentence polarity movie review dataset \n",
    "# https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "!wget https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/polarityData/rt-polaritydata/rt-polarity-pos.txt --directory-prefix=./data/sentiment/\n",
    "!wget https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/polarityData/rt-polaritydata/rt-polarity-neg.txt --directory-prefix=./data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def clean_corpus(fname):\n",
    "    sentences = []\n",
    "    for line in open(fname, encoding=\"utf-8\", errors=\"ignore\"):\n",
    "        # Remove leading/trailing whitespace at the boundaries\n",
    "        line = line.strip()\n",
    "\n",
    "        # Don't include empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Remove non-alphanumeric characters (excl. spaces)\n",
    "        line = \"\".join([char for char in line if char.isalnum() or char == \" \"])\n",
    "\n",
    "        # Remove leading/trailing spaces between words\n",
    "        line = \" \".join([word for word in line.split(\" \") if word.strip()])\n",
    "\n",
    "        # Split into words\n",
    "        words = line.split(\" \")\n",
    "\n",
    "        # Add to collection\n",
    "        sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input\n",
    "sentences_pos = clean_corpus(fname=TRAINING_POS_PATH)\n",
    "sentences_neg = clean_corpus(fname=TRAINING_NEG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Truncate after word threshold\n",
    "sentences_pos = [sentence[:NUM_WORDS] for sentence in sentences_pos]\n",
    "sentences_neg = [sentence[:NUM_WORDS] for sentence in sentences_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine dictionary (0 is used for padding)\n",
    "sentences = sentences_pos + sentences_neg\n",
    "dictionary = [word for sentence in sentences for word in sentence]\n",
    "dictionary = list(set(dictionary))\n",
    "dictionary = dict(zip(dictionary, range(1, len(dictionary) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sentences to sequences of integers\n",
    "sentences_pos = [[dictionary[word] for word in sentence] for sentence in sentences_pos]\n",
    "sentences_neg = [[dictionary[word] for word in sentence] for sentence in sentences_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS: the rock is destined to be the 21st centurys new conan and that hes going to make a splash even greater than arnold schwarzenegger jeanclaud van damme or steven segal\n",
      "NEG: simplistic silly and tedious\n"
     ]
    }
   ],
   "source": [
    "# Check integrity\n",
    "dictionary_inv = {b: a for a, b in dictionary.items()}\n",
    "print(\"POS: \" + \" \".join([dictionary_inv[index] for index in sentences_pos[0]]))\n",
    "print(\"NEG: \" + \" \".join([dictionary_inv[index] for index in sentences_neg[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad sentences to same length from the left side (with zeros)\n",
    "def pad_zeros(sentence):\n",
    "    if len(sentence) == NUM_WORDS:\n",
    "        return sentence\n",
    "    else:\n",
    "        return [0] * (NUM_WORDS - len(sentence)) + sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_pos = [pad_zeros(sentence) for sentence in sentences_pos]\n",
    "sentences_neg = [pad_zeros(sentence) for sentence in sentences_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "data_pos = np.array(sentences_pos, dtype=np.int32)\n",
    "data_pos_labels = np.ones(shape=[len(sentences_pos)], dtype=np.int32)\n",
    "\n",
    "data_neg = np.array(sentences_neg, dtype=np.int32)\n",
    "data_neg_labels = np.zeros(shape=[len(sentences_neg)], dtype=np.int32)\n",
    "\n",
    "data = np.vstack((data_pos, data_neg))\n",
    "data_labels = np.concatenate((data_pos_labels, data_neg_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training/test set\n",
    "np.random.shuffle(data)\n",
    "num_rows = data.shape[0]\n",
    "\n",
    "split_train = int(num_rows * TRAIN_RATIO)\n",
    "train, train_labels = data[:split_train, :], data_labels[:split_train]\n",
    "test, test_labels = data[split_train:, :], data_labels[split_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input function\n",
    "def get_input_fn(x, y=None, batch_size=128, num_epochs=1, shuffle=False):\n",
    "    return tf.estimator.inputs.numpy_input_fn(x={\"x\": x},\n",
    "                                              y=y,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_epochs=num_epochs,\n",
    "                                              shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set model params\n",
    "model_params = {\"learning_rate\": LEARNING_RATE,\n",
    "                \"num_layers\": NUM_LAYERS,\n",
    "                \"num_units\": NUM_UNITS,\n",
    "                \"embedding_size\": EMBEDDING_SIZE,\n",
    "                \"dropout_prob\": DROPOUT_PROB,\n",
    "                \"vocabulary_size\": len(dictionary) + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log loss\n",
    "loss_hook = tf.train.LoggingTensorHook([\"loss\"], every_n_iter=EVERY_N_ITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define LSTM model function\n",
    "def lstm_model_fn(features, labels, mode, params):\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = features[\"x\"]\n",
    "\n",
    "    # Embedding layer\n",
    "    word_embeddings = tf.get_variable(name=\"word_embeddings\",\n",
    "                                      shape=[params[\"vocabulary_size\"], params[\"embedding_size\"]],\n",
    "                                      initializer=tf.random_normal_initializer())\n",
    "    input_layer = tf.nn.embedding_lookup(word_embeddings, input_layer)\n",
    "\n",
    "    # LSTM (with dropout)\n",
    "    basic_lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units=params[\"num_units\"],\n",
    "                                                     activation=tf.nn.tanh)\n",
    "                        for _ in range(params[\"num_layers\"])]\n",
    "    dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(basic_lstm_cell, output_keep_prob=params[\"dropout_prob\"])\n",
    "                          for basic_lstm_cell in basic_lstm_cells]\n",
    "    multi_lstm_cells = tf.nn.rnn_cell.MultiRNNCell(dropout_lstm_cells)\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_lstm_cells, input_layer, dtype=tf.float32)\n",
    "\n",
    "    # Extract final state (last hidden state of sequence of topmost layer)\n",
    "    final_state = states[-1].h\n",
    "\n",
    "    # Fully connected layer (with linear activation)\n",
    "    logits = tf.squeeze(tf.layers.dense(inputs=final_state, units=1, activation=None))\n",
    "\n",
    "    # Define output\n",
    "    sentiment = tf.sigmoid(logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\"sentiment\": sentiment})\n",
    "\n",
    "    # Cast labels\n",
    "    labels = tf.cast(labels, dtype=tf.float32)\n",
    "\n",
    "    # Define loss\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits), name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        tf.summary.scalar(\"cross_entropy\", loss)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'dump/', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Estimator\n",
    "nn = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                            params=model_params,\n",
    "                            model_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into dump/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.702975\n",
      "INFO:tensorflow:loss = 0.702975, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.70295\n",
      "INFO:tensorflow:loss = 0.69077, step = 101 (21.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.12874\n",
      "INFO:tensorflow:loss = 0.697919, step = 201 (24.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.32652\n",
      "INFO:tensorflow:loss = 0.709841, step = 301 (23.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.96112\n",
      "INFO:tensorflow:loss = 0.718091, step = 401 (25.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.16157\n",
      "INFO:tensorflow:loss = 0.733326 (117.877 sec)\n",
      "INFO:tensorflow:loss = 0.733326, step = 501 (24.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.37983\n",
      "INFO:tensorflow:loss = 0.749342, step = 601 (22.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.07951\n",
      "INFO:tensorflow:loss = 0.752695, step = 701 (24.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.47833\n",
      "INFO:tensorflow:loss = 0.761257, step = 801 (28.752 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 833 into dump/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.435147.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1133eaf60>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "nn.train(input_fn=get_input_fn(x=train,\n",
    "                               y=train_labels,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               num_epochs=NUM_EPOCHS,\n",
    "                               shuffle=True),\n",
    "         hooks=[loss_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-05-16:49:02\n",
      "INFO:tensorflow:Restoring parameters from dump/model.ckpt-833\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-05-16:49:03\n",
      "INFO:tensorflow:Saving dict for global step 833: global_step = 833, loss = 1.04197\n",
      "Cross entropy (test set): 1.04\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "eval_dict = nn.evaluate(input_fn=get_input_fn(x=test,\n",
    "                                              y=test_labels,\n",
    "                                              batch_size=test.shape[0]))\n",
    "\n",
    "print(\"Cross entropy (test set): {0:.2f}\".format(eval_dict[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dump/model.ckpt-833\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "prediction = nn.predict(input_fn=get_input_fn(x=test,\n",
    "                                              y=test_labels,\n",
    "                                              batch_size=test.shape[0]))\n",
    "sentiments = np.array([p[\"sentiment\"] for p in prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find indices that would sort array (ascending order)\n",
    "idx = np.argsort(sentiments)\n",
    "idx_lo = idx[:5]\n",
    "idx_hi = idx[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the sentiment of some example sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map word indices back to strings\n",
    "dictionary_inv[0] = \"\"\n",
    "map2str = np.vectorize(dictionary_inv.__getitem__)\n",
    "test_str = map2str(test)\n",
    "test_str = np.apply_along_axis(lambda row: reduce(lambda a, b: a.strip() + \" \" + b.strip(), row), axis=1, arr=test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE:\n",
      "\tyou may think you have figured out the con and the players in this debut film by argentine director fabian bielinsky but while you were thinking someone made off with your wallet ::: 0.521\n",
      "\tits quite an achievement to set and shoot a movie at the cannes film festival and yet fail to capture its visual appeal or its atmosphere ::: 0.527\n",
      "\tit might be tempting to regard mr andrew and his collaborators as oddballs but mr earnharts quizzical charming movie allows us to see them finally as artists ::: 0.533\n",
      "\ta crisply made movie that is no more than mildly amusing ::: 0.536\n",
      "\twith a cast that includes some of the top actors working in independent film lovely amazing involves us because it is so incisive so bleakly amusing about how we go about our lives ::: 0.538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most negative\n",
    "print(\"NEGATIVE:\")\n",
    "for i in idx_lo:\n",
    "    print(\"\\t{} ::: {:.3f}\".format(test_str[i], sentiments[i]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE:\n",
      "\tthe people in abc africa are treated as docile mostly wordless ethnographic extras ::: 0.761\n",
      "\tapparently romantic comedy with a fresh point of view just doesnt figure in the present hollywood program ::: 0.761\n",
      "\tthose unfamiliar with mormon traditions may find the singles ward occasionally bewildering ::: 0.749\n",
      "\tit lacks the compassion goodnatured humor and the level of insight that made eyres first film something of a sleeper success ::: 0.744\n",
      "\tplayfully profound and crazier than michael jackson on the top floor of a skyscraper nursery surrounded by open windows ::: 0.743\n"
     ]
    }
   ],
   "source": [
    "# Most positive\n",
    "print(\"POSITIVE:\")\n",
    "for i in idx_hi[::-1]:\n",
    "    print(\"\\t{} ::: {:.3f}\".format(test_str[i], sentiments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

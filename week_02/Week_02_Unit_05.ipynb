{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Deep Learning with TensorFlow: openSAP \n",
    "\n",
    "## SAP Innovation Center Network\n",
    "```\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Neural Networks with TensorFlow Serving\n",
    "\n",
    "In this notebook, we provide an end-to-end example of training, evaluating and deploying a machine learning application using TensorFlow Serving. We will train and evaluate a deep neural network using the high-level Estimator API which is also used to save the trained model. We will then export the trained model for deployment on TensorFlow Serving. Although TensorFlow Serving is a server, which is called by a client for making inference on new data, we will be simulating both the client and the server in this notebook.\n",
    "\n",
    "#### Data\n",
    "We will be using the Iris data [https://en.wikipedia.org/wiki/Iris_flower_data_set] for classifying inputs into specific flower types. More specifically, the data consists of four different attributes of a flower such as sepal width, sepal length, petal width and petal length which are used to predict the species of a flower. We will use these four attributes to build a classifier that will learn to predict the species of the flower.\n",
    "\n",
    "#### Steps \n",
    "- Load the data from CSV files\n",
    "- Construct a neural network using TensorFlow Estimator API\n",
    "- Train the model using the Iris training data\n",
    "- Evaluate accuracy of trained model on Iris test data\n",
    "- Save model to local file system\n",
    "- Export saved model to be capable of deploying on TensorFlow Serving\n",
    "- Load the exported model into a new session\n",
    "- Predict on new sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code requires TensorFlow v1.3+\n",
      "You have: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print('This code requires TensorFlow v1.3+')\n",
    "print('You have:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "IRIS_TRAINING = \"./data/iris_training.csv\"\n",
    "IRIS_TEST = \"./data/iris_test.csv\"\n",
    "\n",
    "# URLs to download the datasets if not available locally\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "INPUT_TENSOR_NAME = 'inputs'\n",
    "\n",
    "model_path = \"./model\"\n",
    "model_path_serving = \"./model_serving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download the train and test data, if not found on local file system\n",
    "!wget http://download.tensorflow.org/data/iris_test.csv --directory-prefix=./data/\n",
    "!wget http://download.tensorflow.org/data/iris_training.csv --directory-prefix=./data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input functions\n",
    "Input functions are used by the tf.estimator API. These function create TensorFlow operations which generate data for the model to consume. The tf.estimator.inputs.numpy_input_fn is used to produce such an input pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def estimator(model_path):\n",
    "    feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n",
    "    return tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                      hidden_units=[10, 20, 30],\n",
    "                                      n_classes=3,\n",
    "                                      model_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "        filename=IRIS_TRAINING,\n",
    "        target_dtype=np.int,\n",
    "        features_dtype=np.float32)\n",
    "\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={INPUT_TENSOR_NAME: np.array(training_set.data)},\n",
    "        y=np.array(training_set.target),\n",
    "        num_epochs=None,\n",
    "        shuffle=True)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    testing_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "        filename=IRIS_TEST,\n",
    "        target_dtype=np.int,\n",
    "        features_dtype=np.float32)\n",
    "\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={INPUT_TENSOR_NAME: np.array(testing_set.data)},\n",
    "        y=np.array(testing_set.target),\n",
    "        num_epochs=1,\n",
    "        shuffle=False)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    feature_spec = {INPUT_TENSOR_NAME: tf.FixedLenFeature(dtype=tf.float32, shape=[4])}\n",
    "    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_session_config': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_model_dir': './model', '_tf_random_seed': 1, '_save_checkpoints_steps': None}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 257.236, step = 1\n",
      "INFO:tensorflow:global_step/sec: 578.017\n",
      "INFO:tensorflow:loss = 12.7767, step = 101 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.636\n",
      "INFO:tensorflow:loss = 10.0392, step = 201 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.068\n",
      "INFO:tensorflow:loss = 19.3909, step = 301 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.744\n",
      "INFO:tensorflow:loss = 6.35206, step = 401 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 671.089\n",
      "INFO:tensorflow:loss = 5.87247, step = 501 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 699.27\n",
      "INFO:tensorflow:loss = 7.05492, step = 601 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 714.251\n",
      "INFO:tensorflow:loss = 14.1658, step = 701 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 666.631\n",
      "INFO:tensorflow:loss = 4.08404, step = 801 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 751.845\n",
      "INFO:tensorflow:loss = 11.3729, step = 901 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.583\n",
      "INFO:tensorflow:loss = 3.86785, step = 1001 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.515\n",
      "INFO:tensorflow:loss = 6.80298, step = 1101 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.002\n",
      "INFO:tensorflow:loss = 11.207, step = 1201 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.434\n",
      "INFO:tensorflow:loss = 2.31211, step = 1301 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.679\n",
      "INFO:tensorflow:loss = 5.87181, step = 1401 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.892\n",
      "INFO:tensorflow:loss = 10.1189, step = 1501 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.682\n",
      "INFO:tensorflow:loss = 6.90865, step = 1601 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.109\n",
      "INFO:tensorflow:loss = 9.17644, step = 1701 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.785\n",
      "INFO:tensorflow:loss = 5.66111, step = 1801 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.012\n",
      "INFO:tensorflow:loss = 11.14, step = 1901 (0.162 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into ./model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.14073.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-31-21:23:05\n",
      "INFO:tensorflow:Restoring parameters from ./model\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-31-21:23:06\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.966667, average_loss = 0.0510357, global_step = 2000, loss = 1.53107\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = estimator(model_path)\n",
    "\n",
    "# Train model using the train input function, for 2000 steps\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)\n",
    "\n",
    "# Evaluate accuracy of the trained model using the test input pipeline\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting saved model\n",
    "The Estimator API saves the model in a format that can be used to resume training or for inference. However, we might also want to create a service from the trained model that would take in requests and return results. Such a service can either be run locally on our machine, or be deployed on a server or scalably in the cloud.\n",
    "\n",
    "In the following cells, we will export the model and run such a service locally and perform inference on new test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\\model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./model_serving\\\\1509485922\\\\saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "# Export trained model, returns the path to the saved model\n",
    "save_path = classifier.export_savedmodel(model_path_serving, serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create methods that would maintain the session using the model path as an id\n",
    "sess_dict = {}\n",
    "def get_session(model_id):\n",
    "    global sess_dict\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_dict[model_id] = tf.Session(config=config)\n",
    "    return sess_dict[model_id]\n",
    "\n",
    "def load_tf_model(model_path):\n",
    "    sess = get_session(model_path)\n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./model_serving\\\\1509485922\\\\variables\\\\variables'\n"
     ]
    }
   ],
   "source": [
    "# Load the exported model and restore the session\n",
    "sess = load_tf_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging the restored graph\n",
    "In most cases, the exported graphs would be provided as models to be served. It is hence a good practice to know operation name of the graph that would accept the input and the operation name that can be used to obtain the final output scores / probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step/Initializer/zeros\n",
      "global_step\n",
      "global_step/Assign\n",
      "global_step/read\n",
      "input_example_tensor\n",
      "ParseExample/Const\n",
      "ParseExample/ParseExample/names\n",
      "ParseExample/ParseExample/dense_keys_0\n",
      "ParseExample/ParseExample\n",
      "dnn/input_from_feature_columns/input_layer/inputs/Shape\n",
      "dnn/input_from_feature_columns/input_layer/inputs/strided_slice/stack\n",
      "dnn/input_from_feature_columns/input_layer/inputs/strided_slice/stack_1\n",
      "dnn/input_from_feature_columns/input_layer/inputs/strided_slice/stack_2\n",
      "dnn/input_from_feature_columns/input_layer/inputs/strided_slice\n",
      "dnn/input_from_feature_columns/input_layer/inputs/Reshape/shape/1\n",
      "dnn/input_from_feature_columns/input_layer/inputs/Reshape/shape\n",
      "dnn/input_from_feature_columns/input_layer/inputs/Reshape\n",
      "dnn/input_from_feature_columns/input_layer/concat/concat_dim\n",
      "dnn/input_from_feature_columns/input_layer/concat\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/shape\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/min\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/max\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/RandomUniform\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/sub\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform/mul\n",
      "dnn/hiddenlayer_0/kernel/part_0/Initializer/random_uniform\n",
      "dnn/hiddenlayer_0/kernel/part_0\n",
      "dnn/hiddenlayer_0/kernel/part_0/Assign\n",
      "dnn/hiddenlayer_0/kernel/part_0/read\n",
      "dnn/hiddenlayer_0/bias/part_0/Initializer/zeros\n",
      "dnn/hiddenlayer_0/bias/part_0\n",
      "dnn/hiddenlayer_0/bias/part_0/Assign\n",
      "dnn/hiddenlayer_0/bias/part_0/read\n",
      "dnn/hiddenlayer_0/kernel\n",
      "dnn/hiddenlayer_0/MatMul\n",
      "dnn/hiddenlayer_0/bias\n",
      "dnn/hiddenlayer_0/BiasAdd\n",
      "dnn/hiddenlayer_0/Relu\n",
      "dnn/zero_fraction/zero\n",
      "dnn/zero_fraction/Equal\n",
      "dnn/zero_fraction/Cast\n",
      "dnn/zero_fraction/Const\n",
      "dnn/zero_fraction/Mean\n",
      "dnn/dnn/hiddenlayer_0/fraction_of_zero_values/tags\n",
      "dnn/dnn/hiddenlayer_0/fraction_of_zero_values\n",
      "dnn/dnn/hiddenlayer_0/activation/tag\n",
      "dnn/dnn/hiddenlayer_0/activation\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/shape\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/min\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/max\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/RandomUniform\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/sub\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform/mul\n",
      "dnn/hiddenlayer_1/kernel/part_0/Initializer/random_uniform\n",
      "dnn/hiddenlayer_1/kernel/part_0\n",
      "dnn/hiddenlayer_1/kernel/part_0/Assign\n",
      "dnn/hiddenlayer_1/kernel/part_0/read\n",
      "dnn/hiddenlayer_1/bias/part_0/Initializer/zeros\n",
      "dnn/hiddenlayer_1/bias/part_0\n",
      "dnn/hiddenlayer_1/bias/part_0/Assign\n",
      "dnn/hiddenlayer_1/bias/part_0/read\n",
      "dnn/hiddenlayer_1/kernel\n",
      "dnn/hiddenlayer_1/MatMul\n",
      "dnn/hiddenlayer_1/bias\n",
      "dnn/hiddenlayer_1/BiasAdd\n",
      "dnn/hiddenlayer_1/Relu\n",
      "dnn/zero_fraction_1/zero\n",
      "dnn/zero_fraction_1/Equal\n",
      "dnn/zero_fraction_1/Cast\n",
      "dnn/zero_fraction_1/Const\n",
      "dnn/zero_fraction_1/Mean\n",
      "dnn/dnn/hiddenlayer_1/fraction_of_zero_values/tags\n",
      "dnn/dnn/hiddenlayer_1/fraction_of_zero_values\n",
      "dnn/dnn/hiddenlayer_1/activation/tag\n",
      "dnn/dnn/hiddenlayer_1/activation\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/shape\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/min\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/max\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/RandomUniform\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/sub\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform/mul\n",
      "dnn/hiddenlayer_2/kernel/part_0/Initializer/random_uniform\n",
      "dnn/hiddenlayer_2/kernel/part_0\n",
      "dnn/hiddenlayer_2/kernel/part_0/Assign\n",
      "dnn/hiddenlayer_2/kernel/part_0/read\n",
      "dnn/hiddenlayer_2/bias/part_0/Initializer/zeros\n",
      "dnn/hiddenlayer_2/bias/part_0\n",
      "dnn/hiddenlayer_2/bias/part_0/Assign\n",
      "dnn/hiddenlayer_2/bias/part_0/read\n",
      "dnn/hiddenlayer_2/kernel\n",
      "dnn/hiddenlayer_2/MatMul\n",
      "dnn/hiddenlayer_2/bias\n",
      "dnn/hiddenlayer_2/BiasAdd\n",
      "dnn/hiddenlayer_2/Relu\n",
      "dnn/zero_fraction_2/zero\n",
      "dnn/zero_fraction_2/Equal\n",
      "dnn/zero_fraction_2/Cast\n",
      "dnn/zero_fraction_2/Const\n",
      "dnn/zero_fraction_2/Mean\n",
      "dnn/dnn/hiddenlayer_2/fraction_of_zero_values/tags\n",
      "dnn/dnn/hiddenlayer_2/fraction_of_zero_values\n",
      "dnn/dnn/hiddenlayer_2/activation/tag\n",
      "dnn/dnn/hiddenlayer_2/activation\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/shape\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/min\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/max\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/RandomUniform\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/sub\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform/mul\n",
      "dnn/logits/kernel/part_0/Initializer/random_uniform\n",
      "dnn/logits/kernel/part_0\n",
      "dnn/logits/kernel/part_0/Assign\n",
      "dnn/logits/kernel/part_0/read\n",
      "dnn/logits/bias/part_0/Initializer/zeros\n",
      "dnn/logits/bias/part_0\n",
      "dnn/logits/bias/part_0/Assign\n",
      "dnn/logits/bias/part_0/read\n",
      "dnn/logits/kernel\n",
      "dnn/logits/MatMul\n",
      "dnn/logits/bias\n",
      "dnn/logits/BiasAdd\n",
      "dnn/zero_fraction_3/zero\n",
      "dnn/zero_fraction_3/Equal\n",
      "dnn/zero_fraction_3/Cast\n",
      "dnn/zero_fraction_3/Const\n",
      "dnn/zero_fraction_3/Mean\n",
      "dnn/dnn/logits/fraction_of_zero_values/tags\n",
      "dnn/dnn/logits/fraction_of_zero_values\n",
      "dnn/dnn/logits/activation/tag\n",
      "dnn/dnn/logits/activation\n",
      "dnn/head/logits/Shape\n",
      "dnn/head/logits/assert_rank/rank\n",
      "dnn/head/logits/assert_rank/assert_type/statically_determined_correct_type\n",
      "dnn/head/logits/assert_rank/static_checks_determined_all_ok\n",
      "dnn/head/logits/strided_slice/stack\n",
      "dnn/head/logits/strided_slice/stack_1\n",
      "dnn/head/logits/strided_slice/stack_2\n",
      "dnn/head/logits/strided_slice\n",
      "dnn/head/logits/assert_equal/x\n",
      "dnn/head/logits/assert_equal/Equal\n",
      "dnn/head/logits/assert_equal/Const\n",
      "dnn/head/logits/assert_equal/All\n",
      "dnn/head/logits/assert_equal/Assert/Assert\n",
      "dnn/head/logits\n",
      "dnn/head/predictions/class_ids/dimension\n",
      "dnn/head/predictions/class_ids\n",
      "dnn/head/predictions/ExpandDims/dim\n",
      "dnn/head/predictions/ExpandDims\n",
      "dnn/head/predictions/str_classes\n",
      "dnn/head/predictions/probabilities\n",
      "dnn/head/Shape\n",
      "dnn/head/strided_slice/stack\n",
      "dnn/head/strided_slice/stack_1\n",
      "dnn/head/strided_slice/stack_2\n",
      "dnn/head/strided_slice\n",
      "dnn/head/range/start\n",
      "dnn/head/range/limit\n",
      "dnn/head/range/delta\n",
      "dnn/head/range\n",
      "dnn/head/AsString\n",
      "dnn/head/ExpandDims/dim\n",
      "dnn/head/ExpandDims\n",
      "dnn/head/Tile/multiples/1\n",
      "dnn/head/Tile/multiples\n",
      "dnn/head/Tile\n",
      "save/Const\n",
      "save/StringJoin/inputs_1\n",
      "save/StringJoin\n",
      "save/num_shards\n",
      "save/ShardedFilename/shard\n",
      "save/ShardedFilename\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/MergeV2Checkpoints/checkpoint_prefixes\n",
      "save/MergeV2Checkpoints\n",
      "save/Identity\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/restore_shard\n",
      "save/restore_all\n",
      "init\n",
      "init_all_tables\n",
      "group_deps\n",
      "save_1/Const\n",
      "save_1/StringJoin/inputs_1\n",
      "save_1/StringJoin\n",
      "save_1/num_shards\n",
      "save_1/ShardedFilename/shard\n",
      "save_1/ShardedFilename\n",
      "save_1/SaveV2/tensor_names\n",
      "save_1/SaveV2/shape_and_slices\n",
      "save_1/SaveV2\n",
      "save_1/control_dependency\n",
      "save_1/MergeV2Checkpoints/checkpoint_prefixes\n",
      "save_1/MergeV2Checkpoints\n",
      "save_1/Identity\n",
      "save_1/RestoreV2/tensor_names\n",
      "save_1/RestoreV2/shape_and_slices\n",
      "save_1/RestoreV2\n",
      "save_1/Assign\n",
      "save_1/RestoreV2_1/tensor_names\n",
      "save_1/RestoreV2_1/shape_and_slices\n",
      "save_1/RestoreV2_1\n",
      "save_1/Assign_1\n",
      "save_1/RestoreV2_2/tensor_names\n",
      "save_1/RestoreV2_2/shape_and_slices\n",
      "save_1/RestoreV2_2\n",
      "save_1/Assign_2\n",
      "save_1/RestoreV2_3/tensor_names\n",
      "save_1/RestoreV2_3/shape_and_slices\n",
      "save_1/RestoreV2_3\n",
      "save_1/Assign_3\n",
      "save_1/RestoreV2_4/tensor_names\n",
      "save_1/RestoreV2_4/shape_and_slices\n",
      "save_1/RestoreV2_4\n",
      "save_1/Assign_4\n",
      "save_1/RestoreV2_5/tensor_names\n",
      "save_1/RestoreV2_5/shape_and_slices\n",
      "save_1/RestoreV2_5\n",
      "save_1/Assign_5\n",
      "save_1/RestoreV2_6/tensor_names\n",
      "save_1/RestoreV2_6/shape_and_slices\n",
      "save_1/RestoreV2_6\n",
      "save_1/Assign_6\n",
      "save_1/RestoreV2_7/tensor_names\n",
      "save_1/RestoreV2_7/shape_and_slices\n",
      "save_1/RestoreV2_7\n",
      "save_1/Assign_7\n",
      "save_1/RestoreV2_8/tensor_names\n",
      "save_1/RestoreV2_8/shape_and_slices\n",
      "save_1/RestoreV2_8\n",
      "save_1/Assign_8\n",
      "save_1/restore_shard\n",
      "save_1/restore_all\n"
     ]
    }
   ],
   "source": [
    "# Use the restored graph's get_operations() method to know all the names of the ops available in the current graph\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print (str(op.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x_holder = sess.graph.get_operation_by_name(\"input_example_tensor\").outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_holder = sess.graph.get_operation_by_name(\"dnn/head/predictions/probabilities\").outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on a new set of inputs [6.7, 2.5, 5.8, 1.8] which belongs to class 2\n",
    "float_features = tf.train.Feature(float_list=tf.train.FloatList(value=[6.7, 2.5, 5.8, 1.8]))\n",
    "feature_dict = {\"inputs\": float_features}\n",
    "example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "serialized = example.SerializeToString()\n",
    "score = sess.run([predictions_holder], {input_x_holder: [serialized]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0000', '0.0005', '0.9995']\n"
     ]
    }
   ],
   "source": [
    "print([\"%.4f\" % score[0][0][idx] for idx in range(len(score[0][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Let us print the index of the largest score \n",
    "print(np.argmax(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "We were able to train, save, export and serve a model using TensorFlow's Estimator API and Serving. Although we showed an example of serving the model on the local file system, we can use the same method to deploy to a separate machine and infer the same results by posting the test data using the IP address of that machine."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
